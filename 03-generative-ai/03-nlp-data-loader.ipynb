{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59452ed4",
   "metadata": {},
   "source": [
    "# Creating an NLP Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59af2a3",
   "metadata": {},
   "source": [
    "In PyTorch, the data loader plays an indispensable role in managing this vast amount of data. For natural language processing (NLP) tasks like yours, data often comes in variable lengths due to differing sentence structures and lengths across languages. The data loader efficiently batches these variable-length sequences, ensuring that your models are trained on diverse examples in every iteration. This batching is crucial for harnessing the power of parallel computation on GPUs, thus expediting the training process.\n",
    "\n",
    "Furthermore, the data loader aids in shuffling the data set, which is vital for preventing models from memorizing the sequence of training data and promoting better generalization. Especially for NLP tasks, where data might be ordered or clustered by topics, shuffling ensures that the model remains robust and doesn't develop biases based on the order of input.\n",
    "\n",
    "Lastly, in the world of NLP, preprocessing steps such as tokenization, padding, and numericalization are paramount. The data loader in PyTorch provides hooks that allow us to seamlessly integrate these preprocessing steps, ensuring that the raw textual data is transformed into a format that's amenable for deep learning models.\n",
    "In PyTorch, the data loader plays an indispensable role in managing this vast amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162f781",
   "metadata": {},
   "source": [
    "**Data set**\n",
    "\n",
    "A data set in **PyTorch** is an object that represents a collection of data samples. Each data sample typically consists of one or more input features and their corresponding target labels. You can also use your data set to transform your data as needed.\n",
    "\n",
    "**Data loader**\n",
    "\n",
    "A data loader in **PyTorch** is responsible for efficiently loading and batching data from a data set. It abstracts away the process of iterating over a data set, shuffling, and dividing it into batches for training. In NLP applications, the data loader is used to process and transform your text data, rather than just the data set.\n",
    "\n",
    "Data loaders have several key parameters, including the data set to load from, batch size (determining how many samples per batch), shuffle (whether to shuffle the data for each epoch), and more. Data loaders also provide an iterator interface, making it easy to iterate over batches of data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83406464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torchtext v0.18.0\n",
      "Torch v2.3.1\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print(f\"Torchtext v{torchtext.__version__}\")\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(f\"Torch v{torch.__version__}\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a273854",
   "metadata": {},
   "source": [
    "## Custom data set and data loader in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2ef21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\", 'Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.']\n",
      "[\"Fame's a fickle friend, Harry.\", 'Soon we must all face the choice between what is right and what is easy.']\n",
      "['You are awesome!', 'It is our choices, Harry, that show what we truly are, far more than our abilities.']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, sentences):\n",
    "    self.sentences = sentences\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.sentences[idx]\n",
    "\n",
    "batch_size = 2\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "for batch in dataloader:\n",
    "  print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d53dae",
   "metadata": {},
   "source": [
    "Deep learning models can only comprehend numerical data, and words are meaningless to them. Therefore, the next step is to convert these sentences into tensors. Let's see how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fe1bc",
   "metadata": {},
   "source": [
    "## Creating tensors for custom data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e96b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset Length: 6\n",
      "Sample Items:\n",
      "Item 1: tensor([11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "        43, 61,  9, 44,  0, 14,  9, 33,  1])\n",
      "Item 2: tensor([35,  6, 16,  3, 38, 40,  0,  8,  1])\n",
      "Item 3: tensor([12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "        21,  1])\n",
      "Item 4: tensor([54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1])\n",
      "Item 5: tensor([66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "         2, 12, 64, 17, 26, 65,  1])\n",
      "Item 6: tensor([19,  4, 25, 20])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
    "    \"Fame's a fickle friend, Harry.\",\n",
    "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
    "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
    "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, sentences, tokenizer, vocab):\n",
    "    self.sentences = sentences\n",
    "    self.tokenizer = tokenizer\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    tokens = self.tokenizer(self.sentences[idx])\n",
    "    tensor_indices = [self.vocab[token] for token in tokens]\n",
    "    return torch.tensor(tensor_indices)\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
    "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
    "\n",
    "print('Custom Dataset Length:', len(custom_dataset))\n",
    "print('Sample Items:')\n",
    "for i in range(6):\n",
    "  sample_item = custom_dataset[i]\n",
    "  print(f\"Item {i + 1}: {sample_item}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2c1a4",
   "metadata": {},
   "source": [
    "## Custom collate function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22bcc7",
   "metadata": {},
   "source": [
    "A collate function is employed in the context of data loading and batching in machine learning, particularly when dealing with variable-length data, such as sequences (e.g., text, time series, sequences of events). Its primary purpose is to prepare and format individual data samples (examples) into batches that can be efficiently processed by machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fde54",
   "metadata": {},
   "source": [
    "When batch_first=True, output will be in [batch_size x seq_len] shape, otherwise it will be in [seq_len x batch_size] shape. Some models accept input with [batch_size x seq_len] shape while some other models need the input to be of [seq_len x batch_size] shape. Keep in mind that this parameter takes care of putting the input in the desired shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d910a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "  return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8583be91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'want', 'to', 'know', 'what', 'a', 'man', \"'\", 's', 'like', ',', 'take', 'a', 'good', 'look', 'at', 'how', 'he', 'treats', 'his', 'inferiors', ',', 'not', 'his', 'equals', '.']\n",
      "['fame', \"'\", 's', 'a', 'fickle', 'friend', ',', 'harry', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['it', 'is', 'our', 'choices', ',', 'harry', ',', 'that', 'show', 'what', 'we', 'truly', 'are', ',', 'far', 'more', 'than', 'our', 'abilities', '.']\n",
      "['soon', 'we', 'must', 'all', 'face', 'the', 'choice', 'between', 'what', 'is', 'right', 'and', 'what', 'is', 'easy', '.', ',', ',', ',', ',']\n",
      "['youth', 'can', 'not', 'know', 'how', 'age', 'thinks', 'and', 'feels', '.', 'but', 'old', 'men', 'are', 'guilty', 'if', 'they', 'forget', 'what', 'it', 'was', 'to', 'be', 'young', '.']\n",
      "['you', 'are', 'awesome', '!', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "  for row in batch:\n",
    "    for idx in row:\n",
    "      words = [vocab.get_itos()[idx] for idx in row]\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d84952",
   "metadata": {},
   "source": [
    "Looking into the result, you can see that the first dimension is the batch. For example, first batch is the first sentence: \"['if', 'you', 'want', 'to', 'know', 'what', 'a', 'man', \"'\", 's', 'like', ',', 'take', 'a', 'good', 'look', 'at', 'how', 'he', 'treats', 'his', 'inferiors', ',', 'not', 'his', 'equals', '.']\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b02596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_bfFALSE(batch):\n",
    "  padded_batch = pad_sequence(batch, padding_value=0)\n",
    "  return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9d430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'fame']\n",
      "['you', \"'\"]\n",
      "['want', 's']\n",
      "['to', 'a']\n",
      "['know', 'fickle']\n",
      "['what', 'friend']\n",
      "['a', ',']\n",
      "['man', 'harry']\n",
      "[\"'\", '.']\n",
      "['s', ',']\n",
      "['like', ',']\n",
      "[',', ',']\n",
      "['take', ',']\n",
      "['a', ',']\n",
      "['good', ',']\n",
      "['look', ',']\n",
      "['at', ',']\n",
      "['how', ',']\n",
      "['he', ',']\n",
      "['treats', ',']\n",
      "['his', ',']\n",
      "['inferiors', ',']\n",
      "[',', ',']\n",
      "['not', ',']\n",
      "['his', ',']\n",
      "['equals', ',']\n",
      "['.', ',']\n",
      "['it', 'soon']\n",
      "['is', 'we']\n",
      "['our', 'must']\n",
      "['choices', 'all']\n",
      "[',', 'face']\n",
      "['harry', 'the']\n",
      "[',', 'choice']\n",
      "['that', 'between']\n",
      "['show', 'what']\n",
      "['what', 'is']\n",
      "['we', 'right']\n",
      "['truly', 'and']\n",
      "['are', 'what']\n",
      "[',', 'is']\n",
      "['far', 'easy']\n",
      "['more', '.']\n",
      "['than', ',']\n",
      "['our', ',']\n",
      "['abilities', ',']\n",
      "['.', ',']\n",
      "['youth', 'you']\n",
      "['can', 'are']\n",
      "['not', 'awesome']\n",
      "['know', '!']\n",
      "['how', ',']\n",
      "['age', ',']\n",
      "['thinks', ',']\n",
      "['and', ',']\n",
      "['feels', ',']\n",
      "['.', ',']\n",
      "['but', ',']\n",
      "['old', ',']\n",
      "['men', ',']\n",
      "['are', ',']\n",
      "['guilty', ',']\n",
      "['if', ',']\n",
      "['they', ',']\n",
      "['forget', ',']\n",
      "['what', ',']\n",
      "['it', ',']\n",
      "['was', ',']\n",
      "['to', ',']\n",
      "['be', ',']\n",
      "['young', ',']\n",
      "['.', ',']\n"
     ]
    }
   ],
   "source": [
    "dataloader_bfFalse = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn_bfFALSE)\n",
    "\n",
    "for seq in dataloader_bfFalse:\n",
    "  for row in seq:\n",
    "    words = [vocab.get_itos()[idx] for idx in row]\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db7343",
   "metadata": {},
   "source": [
    "It can be seen that the first dimension is now the sequence instead of batch, which means sentences will break so that each row includes a token from each sequence. For example the first row, (['if', 'fame']), includes the first tokens of all the sequences in that batch. You need to be aware of this standard to avoid any confusion when working with recurrent neural networks (RNNs) and transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6179fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "         43, 61,  9, 44,  0, 14,  9, 33,  1],\n",
      "        [35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "Length of sequences in the batch: 27\n",
      "tensor([[12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "         21,  1],\n",
      "        [54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1,  0,  0,\n",
      "          0,  0]])\n",
      "Length of sequences in the batch: 20\n",
      "tensor([[66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "          2, 12, 64, 17, 26, 65,  1],\n",
      "        [19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "Length of sequences in the batch: 25\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "  print(batch)\n",
    "  print(\"Length of sequences in the batch:\", batch.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb5bac",
   "metadata": {},
   "source": [
    "You will see that each batch has a fixed size for all the sequences within the batch.\n",
    "\n",
    "You also have the option to utilize the collate function for tasks such as tokenization, converting tokenized indices, and transforming the result into a tensor. It's important to note that the original data set remains untouched by these transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb55e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, sentences):\n",
    "    self.sentences = sentences\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.sentences[idx]\n",
    "\n",
    "custom_dataset = CustomDataset(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2128633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):  \n",
    "  tensor_batch = []\n",
    "  for sample in batch:\n",
    "    tokens = tokenizer(sample)\n",
    "    tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "  \n",
    "  # Pad sequences within the batch to have equal lengths using pad_sequence\n",
    "  # batch_first=True ensures that the tensors have shape (batch_size, max_sequence_length)\n",
    "  padded_batch = pad_sequence(tensor_batch, batch_first=True)\n",
    "  return padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3259df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
      "          2, 12, 64, 17, 26, 65,  1],\n",
      "        [19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]])\n",
      "Shape of sample 2\n",
      "tensor([[35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
      "         43, 61,  9, 44,  0, 14,  9, 33,  1]])\n",
      "Shape of sample 2\n",
      "tensor([[54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1,  0,  0,\n",
      "          0,  0],\n",
      "        [12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
      "         21,  1]])\n",
      "Shape of sample 2\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "  dataset=custom_dataset,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True,\n",
    "  collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "  print(batch)\n",
    "  print(\"Shape of sample\", len(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8c73a",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061573b6",
   "metadata": {},
   "source": [
    "Create a data loader with a collate function that processes batches of French text (provided below). Sort the data set on sequences length. Then tokenize, numericalize and pad the sequences. Sorting the sequences will minimize the number of `<PAD>`tokens added to the sequences, which enhances the model's performance. Prepare the data in batches of size 4 and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b42e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model 'fr_core_news_sm' is already installed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "# Install spaCy model if not already installed\n",
    "try:\n",
    "  nlp = spacy.load('fr_core_news_sm')\n",
    "  print(\"spaCy model 'fr_core_news_sm' is already installed\")\n",
    "except OSError:\n",
    "  print(\"Installing spaCy model 'fr_core_news_sm'...\")\n",
    "  subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"fr_core_news_sm\"])\n",
    "  print(\"Model installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d263f0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27,   2,   0],\n",
      "        [ 26,  45,   2],\n",
      "        [ 35,   8,   2],\n",
      "        [ 25, 101,   2]])\n",
      "tensor([[  1, 105,  41,   0],\n",
      "        [  1,   3,  76,   0],\n",
      "        [  1,   3,  82,   0],\n",
      "        [ 11,   4,  74,   2]])\n",
      "tensor([[ 28,   4,  10,   9,   0],\n",
      "        [ 38,  10, 107,   9,   0],\n",
      "        [ 12,  69,  51,  49,   0],\n",
      "        [  1,  16, 103,  17,   0]])\n",
      "tensor([[  1,   3,  14, 100,   0,   0],\n",
      "        [ 37,   4,  19,  92,  95,   7],\n",
      "        [ 33,  71, 122, 117,  52,   2],\n",
      "        [ 32,  85,  42,  80,  87,   0]])\n",
      "tensor([[ 30,  18,  19,  88,  21,   2,   0],\n",
      "        [ 31,  43,   8,  15,  57,  73,   0],\n",
      "        [ 36,  62,  90, 110,  60,  83,   0],\n",
      "        [ 34, 112, 104, 106, 108,  56,   0]])\n",
      "tensor([[ 11,   4, 111,  50,  68,   5,   9,   0],\n",
      "        [  1, 113,  55,   6,  86,  53,  47,   0],\n",
      "        [  1,   3,  98,   5, 116,  99,  66,   2],\n",
      "        [120,  97,  75,   4,   6,  93,  20,   7]])\n",
      "tensor([[  1,   3,  14,  20,  58,  44,   6,  72,   0,   0],\n",
      "        [  1,  63,  40,  13,  89,  67,  13,  79,   0,   0],\n",
      "        [  1,   3,  70,  46,  10,  81,  78,   5,  21,   0],\n",
      "        [ 12, 119,  39,   8,   5,  84,  59,  54, 115,   0]])\n",
      "tensor([[ 29,  24,  96, 109,  48,  61,  94,  18,   6, 118,  23,  65,   7],\n",
      "        [  1,   3,  64,  22,  77,  16,  91,  17, 114, 121,  15, 102,   0]])\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "  \"Ceci est une phrase.\",\n",
    "  \"C'est un autre exemple de phrase.\",\n",
    "  \"Voici une troisième phrase.\",\n",
    "  \"Il fait beau aujourd'hui.\",\n",
    "  \"J'aime beaucoup la cuisine française.\",\n",
    "  \"Quel est ton plat préféré ?\",\n",
    "  \"Je t'adore.\",\n",
    "  \"Bon appétit !\",\n",
    "  \"Je suis en train d'apprendre le français.\",\n",
    "  \"Nous devons partir tôt demain matin.\",\n",
    "  \"Je suis heureux.\",\n",
    "  \"Le film était vraiment captivant !\",\n",
    "  \"Je suis là.\",\n",
    "  \"Je ne sais pas.\",\n",
    "  \"Je suis fatigué après une longue journée de travail.\",\n",
    "  \"Est-ce que tu as des projets pour le week-end ?\",\n",
    "  \"Je vais chez le médecin cet après-midi.\",\n",
    "  \"La musique adoucit les mœurs.\",\n",
    "  \"Je dois acheter du pain et du lait.\",\n",
    "  \"Il y a beaucoup de monde dans cette ville.\",\n",
    "  \"Merci beaucoup !\",\n",
    "  \"Au revoir !\",\n",
    "  \"Je suis ravi de vous rencontrer enfin !\",\n",
    "  \"Les vacances sont toujours trop courtes.\",\n",
    "  \"Je suis en retard.\",\n",
    "  \"Félicitations pour ton nouveau travail !\",\n",
    "  \"Je suis désolé, je ne peux pas venir à la réunion.\",\n",
    "  \"À quelle heure est le prochain train ?\",\n",
    "  \"Bonjour !\",\n",
    "  \"C'est génial !\"\n",
    "]\n",
    "\n",
    "def collate_fn_fr(batch):\n",
    "  tensor_batch = []\n",
    "  for sample in batch:\n",
    "    tokens = tokenizer(sample)\n",
    "    tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "\n",
    "  padded_batch = pad_sequence(tensor_batch, batch_first=True)\n",
    "  return padded_batch\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, corpus))\n",
    "\n",
    "sorted_data = sorted(corpus, key=lambda x: len(tokenizer(x)))\n",
    "dataloader = DataLoader(\n",
    "  sorted_data, \n",
    "  batch_size=4, \n",
    "  shuffle=False, \n",
    "  collate_fn=collate_fn_fr\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "  print(batch) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c3e6e",
   "metadata": {},
   "source": [
    "## Data loader for German-English translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fd9ad",
   "metadata": {},
   "source": [
    "This section sets the stage for German-English machine translation using the torchtext and spaCy libraries. It adjusts data set URLs for the Multi30k data set, configures tokenizers for both languages, and establishes vocabularies with special tokens. This foundation is crucial for building and training effective translation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13c17c",
   "metadata": {},
   "source": [
    "### Translation data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda4deb",
   "metadata": {},
   "source": [
    "Fetch a language translation data set called Multi30k. You will modify its default training and validation URLs, and then retrieve and print the first pair of German-English sentences from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5794fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1\n",
      "Source (de: Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
      "Target (en): Two young, White males are outside near many bushes.\n",
      "sample 2\n",
      "Source (de: Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
      "Target (en): Several men in hard hats are operating a giant pulley system.\n",
      "sample 3\n",
      "Source (de: Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
      "Target (en): A little girl climbing into a wooden playhouse.\n",
      "sample 4\n",
      "Source (de: Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
      "Target (en): A man in a blue shirt is standing on a ladder cleaning a window.\n",
      "sample 5\n",
      "Source (de: Zwei Männer stehen am Herd und bereiten Essen zu.\n",
      "Target (en): Two men are at the stove preparing food.\n"
     ]
    }
   ],
   "source": [
    "multi30k.URL[\"train\"] = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "dataset = iter(train_iter)\n",
    "\n",
    "for n in range(5):\n",
    "  src, tgt = next(dataset)\n",
    "  print(f\"sample {str(n+1)}\")\n",
    "  print(f\"Source ({SRC_LANGUAGE}: {src}\\nTarget ({TGT_LANGUAGE}): {tgt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4efcf",
   "metadata": {},
   "source": [
    "### Tokenizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e10033",
   "metadata": {},
   "source": [
    "The tokenizer, set up using spaCy, breaks down text into smaller units or tokens, facilitating precise language processing and ensuring that words and punctuations are appropriately segmented for the translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cb23172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model 'de_core_news_sm' is already installed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "# Install spaCy model if not already installed\n",
    "try:\n",
    "  nlp = spacy.load('de_core_news_sm')\n",
    "  print(\"spaCy model 'de_core_news_sm' is already installed\")\n",
    "except OSError:\n",
    "  print(\"Installing spaCy model 'de_core_news_sm'...\")\n",
    "  subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"de_core_news_sm\"])\n",
    "  print(\"Model installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70ac711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing spaCy model 'en_core_web_sm'...\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model installed successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  nlp = spacy.load('en_core_web_sm')\n",
    "  print(\"spaCy model 'en_core_web_sm' is already installed\")\n",
    "except OSError:\n",
    "  print(\"Installing spaCy model 'en_core_web_sm'...\")\n",
    "  subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "  print(\"Model installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc4e7f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source German (de): Ein schwarzer Hund und ein gefleckter Hund kämpfen.\n",
      "Target English  (en): A black dog and a spotted dog are fighting\n",
      "['Ein', 'schwarzer', 'Hund', 'und', 'ein', 'gefleckter', 'Hund', 'kämpfen', '.']\n",
      "['A', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting']\n"
     ]
    }
   ],
   "source": [
    "german, english = next(dataset)\n",
    "print(f\"Source German ({SRC_LANGUAGE}): {german}\\nTarget English  ({TGT_LANGUAGE}): { english }\")\n",
    "\n",
    "token_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "print(token_transform['de'](german))\n",
    "print(token_transform['en'](english))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6632f",
   "metadata": {},
   "source": [
    "### Special symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd148f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab304d91",
   "metadata": {},
   "source": [
    "### Tokens to indices transformation (Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3538bde",
   "metadata": {},
   "source": [
    "The code initializes a dictionary vocab_transform and then builds vocabularies for both German (de) and English (en) languages from the ```train_iter dataset``` using the helper ```function yield_tokens```. These vocabularies are then stored in the vocab_transform dictionary. The vocabularies are built with certain constraints like a minimum frequency for tokens and the inclusion of special symbols at the beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62a57090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English text string: A black dog and a spotted dog are fighting\n",
      "English sequence: [6, 26, 34, 11, 4, 1763, 34, 17, 679]\n",
      "German text string: Ein schwarzer Hund und ein gefleckter Hund kämpfen.\n",
      "German sequence: [5, 117, 33, 9, 15, 2715, 33, 384, 4]\n"
     ]
    }
   ],
   "source": [
    "vocab_transform = {}\n",
    "\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "  language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "  for data_sample in data_iter:\n",
    "    yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  train_iterator = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "  sorted_dataset = sorted(train_iterator, key=lambda x: len(x[0].split()))\n",
    "  vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(sorted_dataset, ln),\n",
    "                                                               min_freq=1,\n",
    "                                                               specials=special_symbols,\n",
    "                                                               special_first=True)\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "\n",
    "seq_en = vocab_transform['en'](token_transform['en'](english))\n",
    "print(f\"English text string: {english}\\nEnglish sequence: {seq_en}\")\n",
    "\n",
    "seq_de = vocab_transform['de'](token_transform['de'](german))\n",
    "print(f\"German text string: {german}\\nGerman sequence: {seq_de}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35cb2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f09d606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   2,  679,   17,   34, 1763,    4,   11,   34,   26,    6,    3])\n",
      "tensor([   2,    5,  117,   33,    9,   15, 2715,   33,  384,    4,    3])\n"
     ]
    }
   ],
   "source": [
    "# function to add BOS/EOS, flip source sentence and create tensor for input sequence indices\n",
    "def tensor_transform_s(token_ids: List[int]):\n",
    "  return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                    torch.flip(torch.tensor(token_ids), dims=(0,)),\n",
    "                    torch.tensor([EOS_IDX])))\n",
    "\n",
    "def tensor_transform_t(token_ids: List[int]):\n",
    "  return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                    torch.tensor(token_ids),\n",
    "                    torch.tensor([EOS_IDX])))\n",
    "\n",
    "\n",
    "seq_en = tensor_transform_s(seq_en)\n",
    "print(seq_en)\n",
    "\n",
    "seq_de = tensor_transform_t(seq_de)\n",
    "print(seq_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9818e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "  def func(txt_input):\n",
    "    for transform in transforms:\n",
    "      txt_input = transform(txt_input)\n",
    "    return txt_input\n",
    "  return func\n",
    "\n",
    "text_transform = {}\n",
    "text_transform[SRC_LANGUAGE] = sequential_transforms(\n",
    "                                        token_transform[SRC_LANGUAGE], # Tokenization\n",
    "                                        vocab_transform[SRC_LANGUAGE], # Numericalization\n",
    "                                        tensor_transform_s)            # Add BOS/EOS, create tensor\n",
    "\n",
    "text_transform[TGT_LANGUAGE] = sequential_transforms(\n",
    "                                        token_transform[TGT_LANGUAGE], # Tokenization\n",
    "                                        vocab_transform[TGT_LANGUAGE], # Numericalization\n",
    "                                        tensor_transform_s)            # Add BOS/EOS, create tensor\n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0952f6",
   "metadata": {},
   "source": [
    "### Processing data in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87beefa7",
   "metadata": {},
   "source": [
    "The collate_fn function builds upon the utilities you established earlier. It performs the text_transform to a batch of raw data. Furthermore, it ensures consistent sequence lengths within the batch through padding. This transformation readies the data for input to a transformer model designed for language translation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26786859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/74/cm99k95n3hdfwr88pcmdq7kc0000gn/T/ipykernel_97918/2067767210.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src_sequences = torch.tensor(src_sequences, dtype=torch.int64)\n",
      "/var/folders/74/cm99k95n3hdfwr88pcmdq7kc0000gn/T/ipykernel_97918/2067767210.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt_sequences = torch.tensor(tgt_sequences, dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[    2,     3,     1,     1,     1],\n",
       "         [    2,  5510,     3,     1,     1],\n",
       "         [    2,  5510,     3,     1,     1],\n",
       "         [    2,  1701,     8, 12642,     3]]),\n",
       " tensor([[   2,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "         [   2,    5, 3428,  692,  115, 9953,  172,  259, 4623, 6650,    3],\n",
       "         [   2,    5, 2187, 2808,   71, 3823, 1650, 3913,  110,  216,    3],\n",
       "         [   2,   37,  109,  202, 3398,    6,    3,    1,    1,    1,    1]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "  src_batch, tgt_batch = [], []\n",
    "  for src_sample, tgt_sample in batch:\n",
    "    src_sequences = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "    src_sequences = torch.tensor(src_sequences, dtype=torch.int64)\n",
    "    tgt_sequences = text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\"))\n",
    "    tgt_sequences = torch.tensor(tgt_sequences, dtype=torch.int64)\n",
    "    src_batch.append(src_sequences)\n",
    "    tgt_batch.append(tgt_sequences)\n",
    "  \n",
    "  src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "  tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "  return src_batch.to(device), tgt_batch.to(device)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_iterator = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "sorted_train_iterator = sorted(train_iterator, key=lambda x: len(x[0].split()))\n",
    "train_dataloader = DataLoader(sorted_train_iterator, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              collate_fn=collate_fn, \n",
    "                              drop_last=True)\n",
    "\n",
    "valid_iterator = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "sorted_valid_iterator = sorted(valid_iterator, key=lambda x: len(x[0].split()))\n",
    "valid_dataloader = DataLoader(sorted_valid_iterator, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              collate_fn=collate_fn, \n",
    "                              drop_last=True)\n",
    "\n",
    "src, trg = next(iter(train_dataloader))\n",
    "src, trg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

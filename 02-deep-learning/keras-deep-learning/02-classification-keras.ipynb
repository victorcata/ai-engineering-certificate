{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e1756e",
   "metadata": {},
   "source": [
    "# Classification Models with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1c4370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To suppress the warning messages due to use of CPU architechture for tensoflow.\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460f1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59517e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613151ed",
   "metadata": {},
   "source": [
    "The Keras library conveniently includes the MNIST dataset as part of its API. The dataset is readily divided into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "import urllib.request\n",
    "\n",
    "opener = urllib.request.build_opener(\n",
    "    urllib.request.HTTPSHandler(context=ssl.create_default_context(cafile=certifi.where()))\n",
    ")\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75da5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58914e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332425a",
   "metadata": {},
   "source": [
    "The first number in the output tuple is the number of images, and the other two numbers are the size of the images in datset. So, each image is 28 pixels by 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2ae1e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17c3f02d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f99d9",
   "metadata": {},
   "source": [
    "\n",
    "With conventional neural networks, we cannot feed in the image as input as is. So we need to flatten the images into one-dimensional vectors, each of size 1 x (28 x 28) = 1 x 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4c2fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c6d96",
   "metadata": {},
   "source": [
    "Since pixel values can range from 0 to 255, let's normalize the vectors to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1fe3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa92e09",
   "metadata": {},
   "source": [
    "Finally, before we start building our model, remember that for classification we need to divide our target variable into categories. We use the to_categorical function from the Keras Utilities package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20ab70bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# one hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb6a9d",
   "metadata": {},
   "source": [
    "## Build a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93b3eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(num_pixels,)))\n",
    "    model.add(Dense(num_pixels, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc67c8",
   "metadata": {},
   "source": [
    "## Train and Test the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb9452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9440 - loss: 0.1825 - val_accuracy: 0.9659 - val_loss: 0.1098\n",
      "Epoch 2/10\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9759 - loss: 0.0783 - val_accuracy: 0.9775 - val_loss: 0.0769\n",
      "Epoch 3/10\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9832 - loss: 0.0523 - val_accuracy: 0.9685 - val_loss: 0.1018\n",
      "Epoch 4/10\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9871 - loss: 0.0408 - val_accuracy: 0.9775 - val_loss: 0.0693\n",
      "Epoch 5/10\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9897 - loss: 0.0318 - val_accuracy: 0.9802 - val_loss: 0.0711\n",
      "Epoch 6/10\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9917 - loss: 0.0259 - val_accuracy: 0.9824 - val_loss: 0.0763\n",
      "Epoch 7/10\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9919 - loss: 0.0240 - val_accuracy: 0.9766 - val_loss: 0.0997\n",
      "Epoch 8/10\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9936 - loss: 0.0193 - val_accuracy: 0.9785 - val_loss: 0.0962\n",
      "Epoch 9/10\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9941 - loss: 0.0188 - val_accuracy: 0.9806 - val_loss: 0.0943\n",
      "Epoch 10/10\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9953 - loss: 0.0148 - val_accuracy: 0.9771 - val_loss: 0.1091\n"
     ]
    }
   ],
   "source": [
    "model = classification_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0066fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9771000146865845% \n",
      " Error: 0.022899985313415527\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}% \\n Error: {}'.format(scores[1], 1 - scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e70008",
   "metadata": {},
   "source": [
    "Sometimes, you cannot afford to retrain your model everytime you want to use it, especially if you are limited on computational resources and training your model can take a long time. Therefore, with the Keras library, you can save your model after training. To do that, we use the save method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classification_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4af5f",
   "metadata": {},
   "source": [
    "Since our model contains multidimensional arrays of data, then models are usually saved as .keras files.\n",
    "\n",
    "When you are ready to use your model again, you use the load_model function from keras.saving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4972c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras.saving.load_model('classification_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c509225",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d0e93",
   "metadata": {},
   "source": [
    "Create a neural network model with 6 dense layers and compare its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28603e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 7s - 4ms/step - accuracy: 0.9321 - loss: 0.2236 - val_accuracy: 0.9656 - val_loss: 0.1116\n",
      "Epoch 2/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9699 - loss: 0.1024 - val_accuracy: 0.9745 - val_loss: 0.0834\n",
      "Epoch 3/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9790 - loss: 0.0710 - val_accuracy: 0.9723 - val_loss: 0.0973\n",
      "Epoch 4/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9836 - loss: 0.0571 - val_accuracy: 0.9806 - val_loss: 0.0718\n",
      "Epoch 5/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9860 - loss: 0.0464 - val_accuracy: 0.9782 - val_loss: 0.0806\n",
      "Epoch 6/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9886 - loss: 0.0403 - val_accuracy: 0.9802 - val_loss: 0.0775\n",
      "Epoch 7/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9898 - loss: 0.0352 - val_accuracy: 0.9839 - val_loss: 0.0650\n",
      "Epoch 8/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9912 - loss: 0.0308 - val_accuracy: 0.9775 - val_loss: 0.1024\n",
      "Epoch 9/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9926 - loss: 0.0278 - val_accuracy: 0.9821 - val_loss: 0.0766\n",
      "Epoch 10/10\n",
      "1875/1875 - 6s - 3ms/step - accuracy: 0.9932 - loss: 0.0234 - val_accuracy: 0.9833 - val_loss: 0.0753\n",
      "Accuracy 6-layers: 0.983299970626831% \n",
      " Error: 0.016700029373168945\n"
     ]
    }
   ],
   "source": [
    "model_ex1 = Sequential()\n",
    "model_ex1.add(Input(shape=(num_pixels,)))\n",
    "model_ex1.add(Dense(num_pixels, activation='relu'))\n",
    "model_ex1.add(Dense(100, activation='relu'))\n",
    "model_ex1.add(Dense(100, activation='relu'))\n",
    "model_ex1.add(Dense(100, activation='relu'))\n",
    "model_ex1.add(Dense(100, activation='relu'))\n",
    "model_ex1.add(Dense(num_classes, activation='softmax'))\n",
    "model_ex1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_ex1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
    "scores_ex1 = model_ex1.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Accuracy 6-layers: {}% \\n Error: {}'.format(scores_ex1[1], 1 - scores_ex1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41762a58",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180f192",
   "metadata": {},
   "source": [
    "Now, load the the earlier saved model, train it further for 10 more epochs and check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70938ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9957 - loss: 0.0138 - val_accuracy: 0.9838 - val_loss: 0.0797\n",
      "Epoch 2/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9954 - loss: 0.0147 - val_accuracy: 0.9834 - val_loss: 0.1010\n",
      "Epoch 3/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9958 - loss: 0.0135 - val_accuracy: 0.9777 - val_loss: 0.1083\n",
      "Epoch 4/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9964 - loss: 0.0116 - val_accuracy: 0.9832 - val_loss: 0.0941\n",
      "Epoch 5/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9967 - loss: 0.0111 - val_accuracy: 0.9836 - val_loss: 0.0961\n",
      "Epoch 6/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9965 - loss: 0.0124 - val_accuracy: 0.9819 - val_loss: 0.1095\n",
      "Epoch 7/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9966 - loss: 0.0123 - val_accuracy: 0.9815 - val_loss: 0.1193\n",
      "Epoch 8/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9973 - loss: 0.0091 - val_accuracy: 0.9830 - val_loss: 0.1099\n",
      "Epoch 9/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9973 - loss: 0.0099 - val_accuracy: 0.9823 - val_loss: 0.1086\n",
      "Epoch 10/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9977 - loss: 0.0081 - val_accuracy: 0.9828 - val_loss: 0.1214\n",
      "Epoch 11/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9970 - loss: 0.0113 - val_accuracy: 0.9825 - val_loss: 0.1205\n",
      "Epoch 12/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9974 - loss: 0.0094 - val_accuracy: 0.9799 - val_loss: 0.1323\n",
      "Epoch 13/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9974 - loss: 0.0089 - val_accuracy: 0.9839 - val_loss: 0.1123\n",
      "Epoch 14/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9977 - loss: 0.0090 - val_accuracy: 0.9833 - val_loss: 0.1211\n",
      "Epoch 15/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9981 - loss: 0.0075 - val_accuracy: 0.9792 - val_loss: 0.1749\n",
      "Epoch 16/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9972 - loss: 0.0118 - val_accuracy: 0.9827 - val_loss: 0.1251\n",
      "Epoch 17/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9984 - loss: 0.0054 - val_accuracy: 0.9828 - val_loss: 0.1534\n",
      "Epoch 18/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9975 - loss: 0.0102 - val_accuracy: 0.9817 - val_loss: 0.1519\n",
      "Epoch 19/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9980 - loss: 0.0087 - val_accuracy: 0.9800 - val_loss: 0.1553\n",
      "Epoch 20/20\n",
      "1875/1875 - 5s - 3ms/step - accuracy: 0.9979 - loss: 0.0085 - val_accuracy: 0.9829 - val_loss: 0.1493\n",
      "Accuracy_10_epochs: 0.9771000146865845% \n",
      " Accuracy_20_epochs: 0.9829000234603882\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = keras.saving.load_model('classification_model.keras')\n",
    "pretrained_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=2)\n",
    "\n",
    "scores_20_epochs = pretrained_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Accuracy_10_epochs: {}% \\n Accuracy_20_epochs: {}'.format(scores[1], scores_20_epochs[1]))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

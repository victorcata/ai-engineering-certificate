{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c26571",
   "metadata": {},
   "source": [
    "# Custom Training Loops in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ea21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143b6a4",
   "metadata": {},
   "source": [
    "## Basic custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb61ae0",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8867d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66874d",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8948f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Flatten(input_shape=(28, 28)),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f01aa",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c4a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462405fa",
   "metadata": {},
   "source": [
    "### Implement the custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c472db",
   "metadata": {},
   "source": [
    "- Iterate over the dataset for a specified number of epochs. \n",
    "- Compute the loss and apply gradients to update the model's weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20cf1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 - Step 0: Loss = 2.332484245300293\n",
      "Epoch 1 - Step 200: Loss = 0.34551799297332764\n",
      "Epoch 1 - Step 400: Loss = 0.1508319228887558\n",
      "Epoch 1 - Step 600: Loss = 0.14460636675357819\n",
      "Epoch 1 - Step 800: Loss = 0.14067909121513367\n",
      "Epoch 1 - Step 1000: Loss = 0.470578670501709\n",
      "Epoch 1 - Step 1200: Loss = 0.18002387881278992\n",
      "Epoch 1 - Step 1400: Loss = 0.25883451104164124\n",
      "Epoch 1 - Step 1600: Loss = 0.22765761613845825\n",
      "Epoch 1 - Step 1800: Loss = 0.18212977051734924\n",
      "Start of epoch 2\n",
      "Epoch 2 - Step 0: Loss = 0.07223241031169891\n",
      "Epoch 2 - Step 200: Loss = 0.18540161848068237\n",
      "Epoch 2 - Step 400: Loss = 0.09718567878007889\n",
      "Epoch 2 - Step 600: Loss = 0.0350455641746521\n",
      "Epoch 2 - Step 800: Loss = 0.09738698601722717\n",
      "Epoch 2 - Step 1000: Loss = 0.2148551046848297\n",
      "Epoch 2 - Step 1200: Loss = 0.08518266677856445\n",
      "Epoch 2 - Step 1400: Loss = 0.19933189451694489\n",
      "Epoch 2 - Step 1600: Loss = 0.18989154696464539\n",
      "Epoch 2 - Step 1800: Loss = 0.09375859797000885\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "  print(f\"Start of epoch {epoch + 1}\")\n",
    "\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train, training=True)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    if step % 200 == 0:\n",
    "      print(f\"Epoch {epoch + 1} - Step {step}: Loss = {loss_value.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa9dfba",
   "metadata": {},
   "source": [
    "## Adding accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e15d8",
   "metadata": {},
   "source": [
    "Enhance the custom training loop by adding an accuracy metric to monitor model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a7b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "model = Sequential([\n",
    "  Flatten(input_shape=(28, 28)),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2396d3",
   "metadata": {},
   "source": [
    "### Implement the custom training loop with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0ee5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 - Step 0: Loss = 0.04676184803247452 Accuracy = 0.9428402185440063\n",
      "Epoch 1 - Step 200: Loss = 0.11416544020175934 Accuracy = 0.9444523453712463\n",
      "Epoch 1 - Step 400: Loss = 0.09451249986886978 Accuracy = 0.9457134008407593\n",
      "Epoch 1 - Step 600: Loss = 0.045429475605487823 Accuracy = 0.9471529722213745\n",
      "Epoch 1 - Step 800: Loss = 0.04518900811672211 Accuracy = 0.9483904838562012\n",
      "Epoch 1 - Step 1000: Loss = 0.11729727685451508 Accuracy = 0.9496224522590637\n",
      "Epoch 1 - Step 1200: Loss = 0.05716107040643692 Accuracy = 0.950717031955719\n",
      "Epoch 1 - Step 1400: Loss = 0.07786379754543304 Accuracy = 0.951738715171814\n",
      "Epoch 1 - Step 1600: Loss = 0.10416413843631744 Accuracy = 0.9526432156562805\n",
      "Epoch 1 - Step 1800: Loss = 0.030319634824991226 Accuracy = 0.9536569714546204\n",
      "Start of epoch 2\n",
      "Epoch 2 - Step 0: Loss = 0.027988862246274948 Accuracy = 1.0\n",
      "Epoch 2 - Step 200: Loss = 0.06710056960582733 Accuracy = 0.9836753606796265\n",
      "Epoch 2 - Step 400: Loss = 0.07618517428636551 Accuracy = 0.9820760488510132\n",
      "Epoch 2 - Step 600: Loss = 0.03330019861459732 Accuracy = 0.9831531047821045\n",
      "Epoch 2 - Step 800: Loss = 0.02590877376496792 Accuracy = 0.9827949404716492\n",
      "Epoch 2 - Step 1000: Loss = 0.10773850977420807 Accuracy = 0.9828608632087708\n",
      "Epoch 2 - Step 1200: Loss = 0.028832197189331055 Accuracy = 0.9825665950775146\n",
      "Epoch 2 - Step 1400: Loss = 0.02587265707552433 Accuracy = 0.9826463460922241\n",
      "Epoch 2 - Step 1600: Loss = 0.06670599430799484 Accuracy = 0.9825304746627808\n",
      "Epoch 2 - Step 1800: Loss = 0.02100554294884205 Accuracy = 0.9827179312705994\n",
      "Start of epoch 3\n",
      "Epoch 3 - Step 0: Loss = 0.022008037194609642 Accuracy = 1.0\n",
      "Epoch 3 - Step 200: Loss = 0.042586907744407654 Accuracy = 0.9886505007743835\n",
      "Epoch 3 - Step 400: Loss = 0.06127506494522095 Accuracy = 0.9879987239837646\n",
      "Epoch 3 - Step 600: Loss = 0.023391736671328545 Accuracy = 0.9888727068901062\n",
      "Epoch 3 - Step 800: Loss = 0.017781049013137817 Accuracy = 0.9882568717002869\n",
      "Epoch 3 - Step 1000: Loss = 0.10496829450130463 Accuracy = 0.9885427355766296\n",
      "Epoch 3 - Step 1200: Loss = 0.032720450311899185 Accuracy = 0.9879787564277649\n",
      "Epoch 3 - Step 1400: Loss = 0.010140445083379745 Accuracy = 0.988044261932373\n",
      "Epoch 3 - Step 1600: Loss = 0.04018612951040268 Accuracy = 0.9879567623138428\n",
      "Epoch 3 - Step 1800: Loss = 0.014755875803530216 Accuracy = 0.9879580736160278\n",
      "Start of epoch 4\n",
      "Epoch 4 - Step 0: Loss = 0.03951471298933029 Accuracy = 0.96875\n",
      "Epoch 4 - Step 200: Loss = 0.031467005610466 Accuracy = 0.990360677242279\n",
      "Epoch 4 - Step 400: Loss = 0.06272528320550919 Accuracy = 0.9908042550086975\n",
      "Epoch 4 - Step 600: Loss = 0.027853958308696747 Accuracy = 0.9917325377464294\n",
      "Epoch 4 - Step 800: Loss = 0.015116243623197079 Accuracy = 0.9913779497146606\n",
      "Epoch 4 - Step 1000: Loss = 0.07613496482372284 Accuracy = 0.9918519258499146\n",
      "Epoch 4 - Step 1200: Loss = 0.018695002421736717 Accuracy = 0.9913613796234131\n",
      "Epoch 4 - Step 1400: Loss = 0.0036724121309816837 Accuracy = 0.9912562370300293\n",
      "Epoch 4 - Step 1600: Loss = 0.01908489502966404 Accuracy = 0.9912750124931335\n",
      "Epoch 4 - Step 1800: Loss = 0.014869250357151031 Accuracy = 0.991324245929718\n",
      "Start of epoch 5\n",
      "Epoch 5 - Step 0: Loss = 0.043941251933574677 Accuracy = 0.96875\n",
      "Epoch 5 - Step 200: Loss = 0.010617056861519814 Accuracy = 0.9925373196601868\n",
      "Epoch 5 - Step 400: Loss = 0.049190372228622437 Accuracy = 0.9927524924278259\n",
      "Epoch 5 - Step 600: Loss = 0.0254453644156456 Accuracy = 0.9941243529319763\n",
      "Epoch 5 - Step 800: Loss = 0.013571114279329777 Accuracy = 0.9944210648536682\n",
      "Epoch 5 - Step 1000: Loss = 0.046885695308446884 Accuracy = 0.9947552680969238\n",
      "Epoch 5 - Step 1200: Loss = 0.012442786246538162 Accuracy = 0.9944837689399719\n",
      "Epoch 5 - Step 1400: Loss = 0.003445688169449568 Accuracy = 0.9943121075630188\n",
      "Epoch 5 - Step 1600: Loss = 0.015797026455402374 Accuracy = 0.9941638112068176\n",
      "Epoch 5 - Step 1800: Loss = 0.0061264727264642715 Accuracy = 0.9941872358322144\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "  print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train, training=True)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "    if step % 200 == 0:\n",
    "      print(f\"Epoch {epoch + 1} - Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}\")\n",
    "\n",
    "  accuracy_metric.reset_state()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fc76d",
   "metadata": {},
   "source": [
    "## Custom callback for advanced logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f25eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "model = Sequential([\n",
    "  Flatten(input_shape=(28, 28)),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd806d",
   "metadata": {},
   "source": [
    "### Implement the custom training loop with custom callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5feda61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    logs = logs or {}\n",
    "    print(f\"End of epoch {epoch + 1}, loss: {logs.get('loss')}, accuracy: {logs.get('accuracy')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a1d2adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.3126721382141113 Accuracy = 0.0625\n",
      "Epoch 1 Step 200: Loss = 0.4393109083175659 Accuracy = 0.8289800882339478\n",
      "Epoch 1 Step 400: Loss = 0.18942619860172272 Accuracy = 0.8644015192985535\n",
      "Epoch 1 Step 600: Loss = 0.1980287730693817 Accuracy = 0.8800436854362488\n",
      "Epoch 1 Step 800: Loss = 0.17768359184265137 Accuracy = 0.8926342129707336\n",
      "Epoch 1 Step 1000: Loss = 0.4339821934700012 Accuracy = 0.9004745483398438\n",
      "Epoch 1 Step 1200: Loss = 0.1382765918970108 Accuracy = 0.9072127342224121\n",
      "Epoch 1 Step 1400: Loss = 0.24768947064876556 Accuracy = 0.9124955534934998\n",
      "Epoch 1 Step 1600: Loss = 0.20073220133781433 Accuracy = 0.9161266684532166\n",
      "Epoch 1 Step 1800: Loss = 0.16371320188045502 Accuracy = 0.9203394055366516\n",
      "End of epoch 1, loss: 0.04219643771648407, accuracy: 0.9223166704177856\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.06693096458911896 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.21642707288265228 Accuracy = 0.96159827709198\n",
      "Epoch 2 Step 400: Loss = 0.1147642657160759 Accuracy = 0.958463191986084\n",
      "Epoch 2 Step 600: Loss = 0.05495116114616394 Accuracy = 0.9606385231018066\n",
      "Epoch 2 Step 800: Loss = 0.07904938608407974 Accuracy = 0.961376428604126\n",
      "Epoch 2 Step 1000: Loss = 0.31849944591522217 Accuracy = 0.9620691537857056\n",
      "Epoch 2 Step 1200: Loss = 0.07676058262586594 Accuracy = 0.9628694653511047\n",
      "Epoch 2 Step 1400: Loss = 0.1246500313282013 Accuracy = 0.9636866450309753\n",
      "Epoch 2 Step 1600: Loss = 0.1527245044708252 Accuracy = 0.9634017944335938\n",
      "Epoch 2 Step 1800: Loss = 0.07659731805324554 Accuracy = 0.9640477299690247\n",
      "End of epoch 2, loss: 0.04172129184007645, accuracy: 0.9646833539009094\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b81c8b",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa6c08",
   "metadata": {},
   "source": [
    "Implement a basic custom training loop to train a simple neural network on the MNIST dataset. \n",
    "\n",
    "\n",
    "- Set up the environment and load the dataset. \n",
    "\n",
    "- Define the model with a Flatten layer and two Dense layers. \n",
    "\n",
    "- Define the loss function and optimizer. \n",
    "\n",
    "- Implement a custom training loop to iterate over the dataset, compute the loss, and update the model's weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3674d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.04062194377183914\n",
      "Epoch 2 - Loss: 0.042877212166786194\n",
      "Epoch 3 - Loss: 0.04047567397356033\n",
      "Epoch 4 - Loss: 0.033673569560050964\n",
      "Epoch 5 - Loss: 0.01646129973232746\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "model = Sequential([\n",
    "  Flatten(input_shape=(28, 28)),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(5):\n",
    "  for x_batch, y_batch in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch, training=True)\n",
    "      loss = loss_fn(y_batch, logits)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "  print(f'Epoch {epoch + 1} - Loss: {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7e8b0",
   "metadata": {},
   "source": [
    "## Exercise 2: Adding accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c23ae",
   "metadata": {},
   "source": [
    "Objective: Enhance the custom training loop by adding an accuracy metric to monitor model performance. \n",
    "\n",
    "\n",
    "1. Set up the environment and define the model, loss function, and optimizer. \n",
    "\n",
    "2. Add Sparse Categorical Accuracy as a metric. \n",
    "\n",
    "3. Implement the custom training loop with accuracy tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4ca6850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.03837450221180916 Accuracy = 0.9246500134468079\n",
      "Epoch 2: Loss = 0.044245507568120956 Accuracy = 0.9652500152587891\n",
      "Epoch 3: Loss = 0.05348261445760727 Accuracy = 0.9758666753768921\n",
      "Epoch 4: Loss = 0.03362075984477997 Accuracy = 0.982450008392334\n",
      "Epoch 5: Loss = 0.017705854028463364 Accuracy = 0.987333357334137\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Step 4: Implement the Custom Training Loop with Accuracy Tracking\n",
    "epochs = 5 \n",
    "for epoch in range(epochs): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.numpy()} Accuracy = {accuracy_metric.result().numpy()}') \n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a81c75",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fbc55a",
   "metadata": {},
   "source": [
    "Implement a custom callback to log additional metrics and information during training. \n",
    "\n",
    "\n",
    "1. Set up the environment and define the model, loss function, optimizer, and metric. \n",
    "\n",
    "2. Create a custom callback to log additional metrics at the end of each epoch. \n",
    "\n",
    "3. Implement the custom training loop with the custom callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f444d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1, loss: 0.029754186049103737, accuracy: 0.9248999953269958\n",
      "End of epoch 2, loss: 0.030950594693422318, accuracy: 0.965666651725769\n",
      "End of epoch 3, loss: 0.045021310448646545, accuracy: 0.9767833352088928\n",
      "End of epoch 4, loss: 0.048431504517793655, accuracy: 0.9832000136375427\n",
      "End of epoch 5, loss: 0.02104269526898861, accuracy: 0.987583339214325\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train = x_train / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) \n",
    "\n",
    "# Step 2: Define the Model\n",
    "model = Sequential([ \n",
    "    tf.keras.Input(shape=(28, 28)),\n",
    "    Flatten(), \n",
    "    Dense(128, activation='relu'), \n",
    "    Dense(10) \n",
    "]) \n",
    "\n",
    "# Step 3: Define Loss Function, Optimizer, and Metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "\n",
    "# Step 4: Implement the Custom Callback\n",
    "class CustomCallback(Callback): \n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}') \n",
    "\n",
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "custom_callback = CustomCallback() \n",
    "\n",
    "for epoch in range(5): \n",
    "    for x_batch, y_batch in train_dataset: \n",
    "        with tf.GradientTape() as tape: \n",
    "            logits = model(x_batch, training=True) \n",
    "            loss = loss_fn(y_batch, logits) \n",
    "        grads = tape.gradient(loss, model.trainable_weights) \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        accuracy_metric.update_state(y_batch, logits) \n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss.numpy(), 'accuracy': accuracy_metric.result().numpy()}) \n",
    "    accuracy_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09181423",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13139bb1",
   "metadata": {},
   "source": [
    "Add functionality to save the results of each hyperparameter tuning iteration as JSON files in a specified directory. \n",
    "\n",
    "\n",
    "Modify the tuning loop to save each iteration's results as JSON files.\n",
    "\n",
    "Specify the directory where these JSON files will be stored for easier retrieval and analysis of tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c13c0b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 01s]\n",
      "val_accuracy: 0.9700000286102295\n",
      "\n",
      "Best val_accuracy So Far: 0.9900000095367432\n",
      "Total elapsed time: 00h 00m 09s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 2: Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 3: Initialize a Keras Tuner RandomSearch tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_results',\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Step 4: Run the tuner search (make sure the data is correct)\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "\n",
    "# Step 5: Save the tuning results as JSON files\n",
    "try:\n",
    "    for i in range(10):\n",
    "        # Fetch the best hyperparameters from the tuner\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        \n",
    "        # Results dictionary to save hyperparameters and score\n",
    "        results = {\n",
    "            \"trial\": i + 1,\n",
    "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
    "            \"score\": None  # Add any score or metrics if available\n",
    "        }\n",
    "\n",
    "        # Save the results as JSON\n",
    "        with open(os.path.join('tuner_results', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Tuning process has not completed or no results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afebb2",
   "metadata": {},
   "source": [
    "> __num_trials__ specifies the number of top hyperparameter sets to return. Setting num_trials=1 means that it will return only the best set of hyperparameters found during the tuning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c085508",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6eb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05a670",
   "metadata": {},
   "source": [
    "The sys.setrecursionlimit function is used to increase the recursion limit, which helps prevent potential recursion errors when running complex models with deep nested functions or when using certain libraries like TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8401d4",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde53578",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_all, y_all), _ = mnist.load_data()\n",
    "\n",
    "x_all = x_all.reshape((x_all.shape[0], -1)).astype('float32') / 255.0\n",
    "\n",
    "# Split into train+val and test (80/20)\n",
    "x_temp, x_test, y_temp, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train+val into train and validation (75/25 of 80% = 60/20 overall)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c7883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Validation data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "\n",
    "print (f'Training data shape: {x_train.shape}')\n",
    "print(f'Validation data shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87c5d8",
   "metadata": {},
   "source": [
    "## Defining the model with hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085ba38",
   "metadata": {},
   "source": [
    "Define a model-building function that uses the `HyperParameters` object to specify the number of units in a dense layer and the learning rate. This function returns a compiled Keras model that is ready for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e8e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "  model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  model.compile(\n",
    "    optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "  )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a601c1",
   "metadata": {},
   "source": [
    "## Configuring the hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a41b2",
   "metadata": {},
   "source": [
    "This exercise guides you through configuring Keras Tuner. You create a `RandomSearch` tuner, specifying the model-building function, the optimization objective, the number of trials, and the directory for storing results. The search space summary provides an overview of the hyperparameters being tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a83ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "  build_model,\n",
    "  objective='val_accuracy',\n",
    "  max_trials=10,\n",
    "  executions_per_trial=2,\n",
    "  directory='hyperparameters',\n",
    "  project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77bafe",
   "metadata": {},
   "source": [
    "## Running the hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e083e1",
   "metadata": {},
   "source": [
    "Run the hyperparameter search using the `search` method of the tuner. You provide the training and validation data along with the number of epochs. After the search is complete, the results summary displays the best hyperparameter configurations found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970e832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 25s]\n",
      "val_accuracy: 0.9783499836921692\n",
      "\n",
      "Best val_accuracy So Far: 0.9793500006198883\n",
      "Total elapsed time: 00h 04m 13s\n",
      "Results summary\n",
      "Results in hyperparameters/intro_to_kt\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.0007967394249203055\n",
      "Score: 0.9793500006198883\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.0006384825574016378\n",
      "Score: 0.979200005531311\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.0022039876620071563\n",
      "Score: 0.9786999821662903\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.0018009968632617217\n",
      "Score: 0.9783499836921692\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.00034060994466741175\n",
      "Score: 0.9774500131607056\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.00029871190678941467\n",
      "Score: 0.9737000167369843\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.000163158180275917\n",
      "Score: 0.9726000130176544\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.004116480206370245\n",
      "Score: 0.9715499877929688\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.007718655234935361\n",
      "Score: 0.9688499867916107\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.00353332901342474\n",
      "Score: 0.9616999924182892\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdcbcd5",
   "metadata": {},
   "source": [
    "## Analyzing and using the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9db521",
   "metadata": {},
   "source": [
    "Retrieve the best hyperparameters found during the search and print their values. You then build a model with these optimized hyperparameters and train it on the full training data set. Finally, you evaluate the model’s performance on the test set to ensure that it performs well with the selected hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6bc884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The optimal number of units in the first dense layer is 320.\n",
      "The optimal learning rate for the optimizer is 0.0007967394249203055.\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9268 - loss: 0.2523 - val_accuracy: 0.9607 - val_loss: 0.1368\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9687 - loss: 0.1070 - val_accuracy: 0.9694 - val_loss: 0.1008\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9797 - loss: 0.0690 - val_accuracy: 0.9723 - val_loss: 0.0849\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9850 - loss: 0.0497 - val_accuracy: 0.9737 - val_loss: 0.0854\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9899 - loss: 0.0350 - val_accuracy: 0.9766 - val_loss: 0.0779\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9918 - loss: 0.0265 - val_accuracy: 0.9762 - val_loss: 0.0865\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9939 - loss: 0.0206 - val_accuracy: 0.9782 - val_loss: 0.0769\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9957 - loss: 0.0155 - val_accuracy: 0.9759 - val_loss: 0.0898\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0121 - val_accuracy: 0.9785 - val_loss: 0.0826\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0115 - val_accuracy: 0.9758 - val_loss: 0.0982\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - accuracy: 0.9765 - loss: 0.0855\n",
      "Test accuracy: 0.9764999747276306\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\" \n",
    "The optimal number of units in the first dense layer is {best_hps.get('units')}.\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\") \n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_val, y_val)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc30862",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b3c74",
   "metadata": {},
   "source": [
    "Learn how to set up Keras Tuner and prepare the environment for hyperparameter tuning. \n",
    "\n",
    "1. Install Keras Tuner.\n",
    "2. Import necessary libraries.\n",
    "3. Load and preprocess the MNIST data se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e9af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Validation data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "\n",
    "print(f'Training data shape: {x_train.shape}')\n",
    "print(f'Validation data shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28129abd",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0e2cc",
   "metadata": {},
   "source": [
    "Define a model-building function that uses hyperparameters to configure the model architecture. \n",
    "\n",
    "1. Define a model-building function that uses the `HyperParameters` object to specify the number of units in a dense layer and the learning rate. \n",
    "2. Compile the model with sparse categorical cross-entropy loss and Adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54bcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "  model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  model.compile(\n",
    "    optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "  )\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7bda6",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee58a7d",
   "metadata": {},
   "source": [
    "Set up Keras Tuner to search for the best hyperparameter configuration. \n",
    "\n",
    "1. Create a `RandomSearch` tuner using the model-building function. \n",
    "2. Specify the optimization objective, number of trials, and directory for storing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3b4dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from hyperparameters/intro_to_kt/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "  build_model,\n",
    "  objective='val_accuracy',\n",
    "  max_trials=10,\n",
    "  executions_per_trial=2,\n",
    "  directory='hyperparameters',\n",
    "  project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564df51",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda323bf",
   "metadata": {},
   "source": [
    "Run the hyperparameter search and dispaly the summary of the results.\n",
    "\n",
    "1. Run the hyperparameter search using the `search` method of the tuner. \n",
    "2. Pass in the training data, validation data, and the number of epochs. \n",
    "3. Display a summary of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d470d375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in hyperparameters/intro_to_kt\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.0007967394249203055\n",
      "Score: 0.9793500006198883\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.0006384825574016378\n",
      "Score: 0.979200005531311\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.0022039876620071563\n",
      "Score: 0.9786999821662903\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.0018009968632617217\n",
      "Score: 0.9783499836921692\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.00034060994466741175\n",
      "Score: 0.9774500131607056\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.00029871190678941467\n",
      "Score: 0.9737000167369843\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.000163158180275917\n",
      "Score: 0.9726000130176544\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.004116480206370245\n",
      "Score: 0.9715499877929688\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.007718655234935361\n",
      "Score: 0.9688499867916107\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.00353332901342474\n",
      "Score: 0.9616999924182892\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val)) \n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2338305",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1a67b",
   "metadata": {},
   "source": [
    "Retrieve the best hyperparameters from the search and build a model with these optimized values. \n",
    "\n",
    "1. Retrieve the best hyperparameters using the `get_best_hyperparameters` method. \n",
    "2. Build a model using the best hyperparameters. \n",
    "3. Train the model on the full training data set and evaluate its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af70e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The optimal number of units in the first dense layer is 320. \n",
      "The optimal learning rate for the optimizer is 0.0007967394249203055. \n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9264 - loss: 0.2562 - val_accuracy: 0.9594 - val_loss: 0.1398\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9694 - loss: 0.1031 - val_accuracy: 0.9715 - val_loss: 0.0924\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9800 - loss: 0.0669 - val_accuracy: 0.9729 - val_loss: 0.0903\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0476 - val_accuracy: 0.9769 - val_loss: 0.0758\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9905 - loss: 0.0338 - val_accuracy: 0.9769 - val_loss: 0.0772\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9927 - loss: 0.0253 - val_accuracy: 0.9762 - val_loss: 0.0860\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9943 - loss: 0.0194 - val_accuracy: 0.9743 - val_loss: 0.0911\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9953 - loss: 0.0154 - val_accuracy: 0.9799 - val_loss: 0.0770\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9971 - loss: 0.0104 - val_accuracy: 0.9744 - val_loss: 0.1012\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0116 - val_accuracy: 0.9763 - val_loss: 0.0921\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step - accuracy: 0.9777 - loss: 0.0827\n",
      "Validation accuracy: 0.9776999950408936\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "\n",
    "print(f\"\"\" \n",
    "The optimal number of units in the first dense layer is {best_hps.get('units')}. \n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}. \n",
    "\"\"\") \n",
    "\n",
    "model = tuner.hypermodel.build(best_hps) \n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2) \n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_val, y_val) \n",
    "print(f'Validation accuracy: {val_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
